{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering file for Zodiac project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file generates a dataframe used for the [twitter-zodiac project](github.com/simoneengelbr7twitter-zodiac). The data is gathered from www.trackalytics.com, the Twitter API, and the Wikipedia API. The data consists of Twitter user information and tweets from >2000 of the most popular Twitter users. Based on the Twitter user information, a search of Wikipedia is performed and the birthday of a likely match is extracted. The resulting data should however be cleaned manually. A clean version of the dataframes can be found as a csv file in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import tweepy\n",
    "import twitter\n",
    "from urllib.parse import quote\n",
    "import urllib3\n",
    "import wptools\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most popular Twitter Profiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most popular twitter profiles are scraped from trackanalytics.com. The profiles are listed in a paginated table, and the site has 655 pages as of November 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the most followed twitter profiles by scraping trackanalytics.com ###\n",
    "\n",
    "# trackanalytics 'Most Followed Twitter Profiles' page url\n",
    "url = 'https://www.trackalytics.com/the-most-followed-twitter-profiles/page/'\n",
    "\n",
    "# list of [screen name, username] for each popular page\n",
    "user_list = []\n",
    "\n",
    "# Iterate through site pages\n",
    "for num in range(1, 655):\n",
    "    try:\n",
    "        # Load page\n",
    "        query = url + str(num) + '/'\n",
    "        response = urllib3.PoolManager().request('GET', query).data\n",
    "\n",
    "        # Avoid decoding warnings\n",
    "        FromRaw = lambda r: r if isinstance(r, str) else r.decode('utf-8', 'ignore')\n",
    "\n",
    "        # Parse page html\n",
    "        soup = BeautifulSoup(FromRaw(response), 'html.parser')\n",
    "        \n",
    "        # Extract row elements in table of users\n",
    "        rows = soup.find_all('tr')[1:]\n",
    "        for row in rows:\n",
    "            try:\n",
    "                # Parse row to extract link with twitter user\n",
    "                new = BeautifulSoup(str(row), 'html.parser').find_all('a')[1]\n",
    "                screen_name = new.get('title')\n",
    "                username = \"@\" + new.get('href').split('/')[-2] # get username from link\n",
    "                \n",
    "                # Store user in user_list\n",
    "                user_list.append([screen_name, username])\n",
    "\n",
    "            except Exception as e:\n",
    "                print('link error:', i, e)\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print('page error:', num, e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia page by Twitter user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of Twitter users is enriched with birthday information if a matching Wikipedia page with a birthday can be found.\n",
    "\n",
    "First, a number of methods are defined, and then a list of information for each user is compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to search Wikipedia for a given search word. Returns list of wiki-links\n",
    "\n",
    "def search_wiki(search_word):\n",
    "    # Compile search query\n",
    "    query = 'https://en.wikipedia.org/w/api.php?action=query&format=json&list=search&srsearch=' + quote(search_word.replace(' ', '_'))\n",
    "    try:\n",
    "        # Query wikipedia\n",
    "        wikiresponse = urllib3.PoolManager().request('GET', query)\n",
    "        wikijson2 = json.loads(wikiresponse.data)\n",
    "        \n",
    "        # Compile list of wikilink results\n",
    "        results_list = []\n",
    "        \n",
    "        # extract wikilinks from search results in JSON format\n",
    "        for i in wikijson2['query']['search']:\n",
    "            try:\n",
    "                results_list.append(i['title'])\n",
    "            except:\n",
    "                print('no results:', i)\n",
    "                continue\n",
    "        return results_list\n",
    "    except Exception as e:\n",
    "        print('Query error:', search_word, e)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to retrieve a birthday from the Wikipedia page with the supplied wiki-link, returns date object.\n",
    "# Throws error if no parsable birthday is found\n",
    "\n",
    "def query_birthday(wiki_link):\n",
    "    # wptools is used to read wikipedia infoboxes, as these are often inconsistent\n",
    "    page = wptools.page(wiki_link, verbose=False, silent=True)\n",
    "    page.get_parse()\n",
    "\n",
    "    # attempt to extract and birthday from infobox birth_date\n",
    "    birthday = page.data['infobox']['birth_date']\n",
    "    birthday = re.search('\\d{4}\\|\\d+\\|\\d+', birthday).group()\n",
    "    \n",
    "    # parse birthday to date object\n",
    "    date = datetime.datetime.strptime(birthday, \"%Y|%m|%d\").date()\n",
    "    \n",
    "    return date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to determine zodiac sign from date object. Returns the zodiac sign of the given date\n",
    "# Code adapted from: https://www.geeksforgeeks.org/program-display-astrological-sign-zodiac-sign-given-date-birth/\n",
    "\n",
    "def zodiac_sign(date): \n",
    "    month = date.month\n",
    "    day = date.day\n",
    "\n",
    "    if month == 12: \n",
    "        astro_sign = 'Sagittarius' if (day < 22) else 'Capricorn'\n",
    "        \n",
    "    elif month == 1: \n",
    "        astro_sign = 'Capricorn' if (day < 20) else 'Aquarius'\n",
    "        \n",
    "    elif month == 2: \n",
    "        astro_sign = 'Aquarius' if (day < 19) else 'Pisces'\n",
    "        \n",
    "    elif month == 3: \n",
    "        astro_sign = 'Pisces' if (day < 21) else 'Aries'\n",
    "        \n",
    "    elif month == 4: \n",
    "        astro_sign = 'Aries' if (day < 20) else 'Taurus'\n",
    "        \n",
    "    elif month == 5: \n",
    "        astro_sign = 'Taurus' if (day < 21) else 'Gemini'\n",
    "        \n",
    "    elif month == 6: \n",
    "        astro_sign = 'Gemini' if (day < 21) else 'Cancer'\n",
    "        \n",
    "    elif month == 7: \n",
    "        astro_sign = 'Cancer' if (day < 23) else 'Leo'\n",
    "        \n",
    "    elif month == 8: \n",
    "        astro_sign = 'Leo' if (day < 23) else 'Virgo'\n",
    "        \n",
    "    elif month == 9: \n",
    "        astro_sign = 'Virgo' if (day < 23) else 'Libra'\n",
    "        \n",
    "    elif month == 10: \n",
    "        astro_sign = 'Libra' if (day < 23) else 'Scorpio'\n",
    "        \n",
    "    elif month == 11: \n",
    "        astro_sign = 'Scorpio' if (day < 22) else 'Sagittarius'\n",
    "        \n",
    "    return astro_sign "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to determine zodiac element. Returns the element of the given zodiac sign\n",
    "    \n",
    "def zodiac_element(zodiac_sign):\n",
    "    earth = ['Capricorn', 'Taurus', 'Virgo' ]\n",
    "    water = ['Cancer', 'Pisces', 'Scorpio' ]\n",
    "    fire = ['Leo', 'Aries', 'Sagittarius']\n",
    "    air = ['Libra', 'Gemini', 'Aquarius']\n",
    "\n",
    "    if zodiac_sign in earth:\n",
    "        return 'Earth'\n",
    "    elif zodiac_sign in water:\n",
    "        return 'Water'\n",
    "    elif zodiac_sign in fire:\n",
    "        return 'Fire'\n",
    "    elif zodiac_sign in air:\n",
    "        return 'Air'\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the wiki-link, birthday, sign, and element for each user\n",
    "\n",
    "Dictionary of user information\n",
    "info = {}\n",
    "\n",
    "for user in user_list:\n",
    "    # Get name and twitter username from user\n",
    "    name, username = user[:2]\n",
    "\n",
    "    # Find Wiki pages corresponding to twitter username and screen name\n",
    "    pages = (search_wiki(username))\n",
    "    pages += (search_wiki(name))\n",
    "    \n",
    "    # Remove some non-person Wikipedia results (ie. 'List of most followed twitter profiles')\n",
    "    pages = [p for p in pages if 'List of' not in p]\n",
    "\n",
    "    # If twitter name (first and last) appears in searches, these are prioritized\n",
    "    first_name = name.split(' ')[0]\n",
    "    last_name = name.split(' ')[-1]\n",
    "    name_matches = [p for p in pages if first_name.lower() in p.lower() and last_name.lower() in p.lower() and '&' not in p]\n",
    "    \n",
    "    # Sort by length as celebrities often have several related pages, ie. Discography\n",
    "    matches = sorted(name_matches, key=len)\n",
    "\n",
    "    if len(matches) > 0:\n",
    "        wiki_link = matches[0] # best name match\n",
    "    elif len(pages) > 0: \n",
    "        wiki_link = pages[0] # first result from query\n",
    "    else:\n",
    "        # No pages were found\n",
    "        continue\n",
    "\n",
    "    # Extract birthday from wiki-link\n",
    "    try:\n",
    "        birthday = query_birthday(wiki_link)\n",
    "    except Exception as e:\n",
    "        # Initial match did not yield birthday\n",
    "        try:\n",
    "            # Second and last attempt, this time compiling all results in prioritized list\n",
    "            \n",
    "            # If wikilink name (first and last) appears in twitter username, these are also prioritized\n",
    "            username_matches = [p for p in pages if (p.split(' ')[0].lower() in username and p.split(' (')[0].split(' ')[-1].lower() in username) or (p.split(' ')[0].lower() in name and p.split(' (')[0].split(' ')[-1].lower() in name)]\n",
    "            \n",
    "            # Compile list\n",
    "            new_matches = [m for m in username_matches+name_matches+pages if not m is wiki_link]\n",
    "            # select new wiki-link\n",
    "            wiki_link = new_matches[0]\n",
    "    \n",
    "            # Extract birthday from wiki-link\n",
    "            birthday = query_birthday(wiki_link)\n",
    "        except:\n",
    "            # No birthday found, twitter user is dropped\n",
    "            print('FAILURE:' wiki_link, user)\n",
    "            continue\n",
    "\n",
    "            \n",
    "    # Determine zodiac from birthday\n",
    "    zodiac = zodiac_sign(birthday)\n",
    "    \n",
    "    # Determine element from zodiac\n",
    "    element = zodiac_element(zodiac)\n",
    "    \n",
    "    # compile results as list in info dictionary\n",
    "    info[name] = [name, username, birthday, zodiac, element, wiki_link] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the information gathered so far\n",
    "columns = ['Name', 'Handle', 'Birthday', 'ZodiacSign', 'Element', 'WikiLink']\n",
    "df = pd.DataFrame(list(info.values()), columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter API queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data for each user will be enriched with information from Twitter. The Twitter API will be used to query the last 50 tweets from each user. Hereafter the profiles that each user follows, called friends, will be queried. As and the twitter friends are returned as Twitter user ids, the twitter ID of each user will be queried so friendships within the dataset can be determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the twitter API credentials (not shown here for privacy)\n",
    "\n",
    "with open(\"twitter_credentials.json\", \"r\") as file:\n",
    "    creds = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query tweets from twitter API using tweepy package\n",
    "\n",
    "# Initialize API\n",
    "auth = tweepy.OAuthHandler(creds['CONSUMER_KEY'], creds['CONSUMER_SECRET'])\n",
    "auth.set_access_token(creds['ACCESS_TOKEN'], creds['ACCESS_SECRET'])\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "tweets = {}\n",
    "\n",
    "# Loop through each Twitter username in dataframe\n",
    "for twitter_user in df.Handle:\n",
    "    \n",
    "    tweets[twitter_user] = []\n",
    "    \n",
    "    # Getting the 50 last tweets \n",
    "    for status in tweepy.Cursor(api.user_timeline, id = twitter_user, since=\"2017-10-20\",lang=\"en\", include_rts=False).items(50):\n",
    "        tweets[twitter_user].append(status.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query friends from twitter API\n",
    "# This query can only load 5000 friends at a time, friendships will need to be queried in a loop\n",
    "# There is a rate limit of 15 calls pr 15 minutes\n",
    "\n",
    "# Initialize API\n",
    "twitter = Twitter(auth=OAuth(creds['ACCESS_TOKEN'], creds['ACCESS_SECRET'], creds['CONSUMER_KEY'], creds['CONSUMER_SECRET']))\n",
    "\n",
    "friends = {}\n",
    "\n",
    "# Loop through each Twitter username in dataframe\n",
    "for username in df.Handle:\n",
    "    # Initial query (max size 5000)\n",
    "    try:\n",
    "        query = twitter.friends.ids(screen_name = username)\n",
    "    except TwitterHTTPError: \n",
    "        print(\"Rate limit potentially reached, going to sleep\")\n",
    "        time.sleep(15*60)\n",
    "        # Retry (will fail if user does not exist)\n",
    "        query = twitter.friends.ids(screen_name = username)\n",
    "    \n",
    "    # Compile list of friends\n",
    "    friends[username] = []\n",
    "    \n",
    "    # Query in loop until no more friends left\n",
    "    while len(query[\"ids\"]) > 0:\n",
    "        \n",
    "        # Add friend query IDs to friend list\n",
    "        friends[username] += query[\"ids\"]\n",
    "        try:\n",
    "            # query from 'next cursor'(end) of previous query\n",
    "            query = twitter.friends.ids(cursor = query['next_cursor'], screen_name = username)\n",
    "        except TwitterHTTPError:\n",
    "            print(\"Friends rate limit reached, going to sleep\")\n",
    "            time.sleep(15*60)\n",
    "            query = twitter.friends.ids(cursor = query['next_cursor'], screen_name = username)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query ids from twitter API\n",
    "\n",
    "# Initialize API\n",
    "twitter = Twitter(auth=OAuth(creds['ACCESS_TOKEN'], creds['ACCESS_SECRET'], creds['CONSUMER_KEY'], creds['CONSUMER_SECRET']))\n",
    "\n",
    "\n",
    "# List of twitter usernames without '@'\n",
    "usernames = [handle[1:] for handle in df['Handle']]\n",
    "\n",
    "ids = {}\n",
    "\n",
    "# load usernames in chunks of 100, as API can take 100 usernames in each query\n",
    "chunks = [usernames[x:x+100] for x in range(0, len(usernames), 100)]\n",
    "\n",
    "for chunk in [usernames[x:x+100] for x in range(0, len(usernames), 100)]:\n",
    "    usernames_string = \",\".join(chunk)\n",
    "    \n",
    "    try:\n",
    "        subquery = twitter.users.lookup(screen_name = usernames_string)\n",
    "    except TwitterHTTPError:\n",
    "            print(\"User ID rate limit reached, going to sleep\")\n",
    "        time.sleep(15*60)\n",
    "        subquery = twitter.users.lookup(screen_name = usernames_string)\n",
    "    \n",
    "    # Store each resulting user ID in ids dictionary\n",
    "    for user in subquery:\n",
    "        ids['@' + user['screen_name'].lower()] = user['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the results to the dataframe as new columns\n",
    "df['Tweets'] = df['Handle'].map(tweets)  \n",
    "df['Following'] = df['Handle'].map(friends)\n",
    "df['Id'] = df['Handle'].map(ids)\n",
    "\n",
    "# Save the id as int instead of as double without affecting the NaN\n",
    "df['Id'] = df['Id'].fillna(-1)\n",
    "df['Id'] = df['Id'].astype(int)\n",
    "df['Id'] = df['Id'].replace('-1', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this execution is a dataframe of popular twitter users and their birthday, tweets, and friends. note that the data should be manually cleaned, as the wikipedia page match is erroneous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe as file\n",
    "df.drop(['Unnamed: 0'], axis = 1, inplace =True)\n",
    "df.to_pickle(\"./datafiles/full_df.pkl\")\n",
    "df.to_csv(\"./datafiles/full_df.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
